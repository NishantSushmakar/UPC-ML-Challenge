{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20d811ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/nishantsushmakar/Documents/UPC/UPC-ML-Challenge/venv/lib/python3.13/site-packages (from torch) (4.13.2)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-80.3.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /Users/nishantsushmakar/Documents/UPC/UPC-ML-Challenge/venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /Users/nishantsushmakar/Documents/UPC/UPC-ML-Challenge/venv/lib/python3.13/site-packages (from torchvision) (2.2.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/nishantsushmakar/Documents/UPC/UPC-ML-Challenge/venv/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.7.0-cp313-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.0-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading setuptools-80.3.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, setuptools, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.3.2 jinja2-3.1.6 mpmath-1.3.0 setuptools-80.3.1 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85dbb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sys\n",
    "import os\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6b9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../scripts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5100fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from training import create_groupkfolds\n",
    "from feature_creation import *\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa79950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, log_loss\n",
    ")\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define your neural network\n",
    "class BinaryClassificationNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryClassificationNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# 0-1 loss function\n",
    "def zero_one_loss(y_true, y_pred):\n",
    "    incorrect = (y_true != y_pred).sum()\n",
    "    return incorrect / len(y_true)\n",
    "\n",
    "\n",
    "# Training and validation function\n",
    "def train_and_validate(X_train, y_train, X_val, y_val, epochs=20, batch_size=32, lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    model = BinaryClassificationNN(input_dim).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                  torch.tensor(y_train, dtype=torch.float32))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                torch.tensor(y_val, dtype=torch.float32))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device).unsqueeze(1)\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_inputs = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "        val_labels = torch.tensor(y_val, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "        val_preds = model(val_inputs).cpu().numpy().flatten()\n",
    "        val_preds_labels = (val_preds >= 0.5).astype(int)\n",
    "\n",
    "    y_true = y_val\n",
    "    y_pred = val_preds_labels\n",
    "    y_proba = val_predsx\n",
    "\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
    "    print(\"ROC AUC Score:\", roc_auc_score(y_true, y_proba))\n",
    "    print(\"Log Loss:\", log_loss(y_true, y_proba))\n",
    "    print(\"Zero-One Loss:\", zero_one_loss(y_true, y_pred))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74793012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.TRAINING_DATA_PATH)\n",
    "n_folds = 5\n",
    "df = create_groupkfolds(df, n_folds, 'sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be422177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langauge Feature Started\n",
      "Langauge Feature Ended\n",
      "Graph Features Creation Started\n",
      "Graph Feature Creation Ended\n",
      "Node Features Creation Started\n",
      "Node Features Creation Ended\n",
      "DataFrame Creation Started\n",
      "DataFrame Creation Ended!!\n",
      "One Hot Encoding Started\n",
      "One Hot Encoding created and Saved\n",
      "Langauge Feature Started\n",
      "Langauge Feature Ended\n",
      "Graph Features Creation Started\n",
      "Graph Feature Creation Ended\n",
      "Node Features Creation Started\n",
      "Node Features Creation Ended\n",
      "DataFrame Creation Started\n",
      "DataFrame Creation Ended!!\n",
      "One Hot Encoding Started\n",
      "One Hot Encoding created and Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishantsushmakar/Documents/UPC/UPC-ML-Challenge/venv/lib/python3.13/site-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../resources/nn/scaler_nn_0.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = 0\n",
    "model = 'nn'\n",
    "df_train = df[df.kfold!=fold].reset_index(drop=True)\n",
    "df_valid = df[df.kfold==fold].reset_index(drop=True)\n",
    "\n",
    "if not os.path.exists(f'../resources/{model}'):\n",
    "        os.makedirs(f'../resources/{model}')\n",
    "\n",
    "feature_pipeline = Pipeline(steps=[\n",
    "                (\"Language Features\",LanguageFeature()),\n",
    "                (\"Graph Features\",GraphFeatures()),\n",
    "                (\"Node Features\",NodeFeatures()),\n",
    "                (\"Dataset Creation\",FormatDataFrame()),\n",
    "                (\"Language One Hot Encoding\",LanguageOHE(enc_lan=f\"{model}/lan_encoder_{model}_{fold}.pkl\",\\\n",
    "                                                         enc_lan_family=f\"{model}/lan_family_encoder_{model}_{fold}.pkl\"))\n",
    "            ])\n",
    "\n",
    "train_data = feature_pipeline.fit_transform(df_train) \n",
    "valid_data = feature_pipeline.transform(df_valid)\n",
    "\n",
    "\n",
    "x_train_data = train_data.drop(columns=config.TRAIN_DROP_COLS)\n",
    "y_train_data = train_data.is_root.values\n",
    "\n",
    "\n",
    "x_valid_data = valid_data.drop(columns=config.TRAIN_DROP_COLS)\n",
    "y_valid_data = valid_data.is_root.values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_train_data = scaler.fit_transform(x_train_data)\n",
    "x_valid_data = scaler.transform(x_valid_data)\n",
    "joblib.dump(scaler,os.path.join(config.ONE_HOT_ENCODER_LANGUAGE,f'{model}/scaler_{model}_{fold}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cbae59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/600] Loss: 0.1833\n",
      "Epoch [2/600] Loss: 0.0869\n",
      "Epoch [3/600] Loss: 0.2973\n",
      "Epoch [4/600] Loss: 0.1197\n",
      "Epoch [5/600] Loss: 0.1012\n",
      "Epoch [6/600] Loss: 0.0237\n",
      "Epoch [7/600] Loss: 0.3316\n",
      "Epoch [8/600] Loss: 0.7548\n",
      "Epoch [9/600] Loss: 0.0230\n",
      "Epoch [10/600] Loss: 0.0017\n",
      "Epoch [11/600] Loss: 0.1821\n",
      "Epoch [12/600] Loss: 0.0163\n",
      "Epoch [13/600] Loss: 0.0339\n",
      "Epoch [14/600] Loss: 0.0110\n",
      "Epoch [15/600] Loss: 0.0205\n",
      "Epoch [16/600] Loss: 0.2674\n",
      "Epoch [17/600] Loss: 1.0802\n",
      "Epoch [18/600] Loss: 0.0090\n",
      "Epoch [19/600] Loss: 0.1162\n",
      "Epoch [20/600] Loss: 0.1399\n",
      "Epoch [21/600] Loss: 0.0308\n",
      "Epoch [22/600] Loss: 0.0695\n",
      "Epoch [23/600] Loss: 0.0548\n",
      "Epoch [24/600] Loss: 0.1492\n",
      "Epoch [25/600] Loss: 0.0430\n",
      "Epoch [26/600] Loss: 0.3932\n",
      "Epoch [27/600] Loss: 0.1012\n",
      "Epoch [28/600] Loss: 0.1347\n",
      "Epoch [29/600] Loss: 1.5217\n",
      "Epoch [30/600] Loss: 0.0063\n",
      "Epoch [31/600] Loss: 0.0124\n",
      "Epoch [32/600] Loss: 0.0299\n",
      "Epoch [33/600] Loss: 0.0274\n",
      "Epoch [34/600] Loss: 0.0095\n",
      "Epoch [35/600] Loss: 0.0484\n",
      "Epoch [36/600] Loss: 0.0596\n",
      "Epoch [37/600] Loss: 0.0365\n",
      "Epoch [38/600] Loss: 0.0632\n",
      "Epoch [39/600] Loss: 0.0277\n",
      "Epoch [40/600] Loss: 0.0315\n",
      "Epoch [41/600] Loss: 0.0434\n",
      "Epoch [42/600] Loss: 0.2364\n",
      "Epoch [43/600] Loss: 0.1049\n",
      "Epoch [44/600] Loss: 0.0237\n",
      "Epoch [45/600] Loss: 0.0109\n",
      "Epoch [46/600] Loss: 0.0227\n",
      "Epoch [47/600] Loss: 0.3467\n",
      "Epoch [48/600] Loss: 0.0457\n",
      "Epoch [49/600] Loss: 0.0227\n",
      "Epoch [50/600] Loss: 0.0221\n",
      "Epoch [51/600] Loss: 0.0212\n",
      "Epoch [52/600] Loss: 0.0611\n",
      "Epoch [53/600] Loss: 0.0556\n",
      "Epoch [54/600] Loss: 0.0430\n",
      "Epoch [55/600] Loss: 0.5197\n",
      "Epoch [56/600] Loss: 1.2220\n",
      "Epoch [57/600] Loss: 0.0435\n",
      "Epoch [58/600] Loss: 0.1466\n",
      "Epoch [59/600] Loss: 0.7389\n",
      "Epoch [60/600] Loss: 0.1302\n",
      "Epoch [61/600] Loss: 0.0378\n",
      "Epoch [62/600] Loss: 0.2704\n",
      "Epoch [63/600] Loss: 0.0374\n",
      "Epoch [64/600] Loss: 0.0097\n",
      "Epoch [65/600] Loss: 0.2304\n",
      "Epoch [66/600] Loss: 0.0190\n",
      "Epoch [67/600] Loss: 0.0919\n",
      "Epoch [68/600] Loss: 0.0993\n",
      "Epoch [69/600] Loss: 0.2457\n",
      "Epoch [70/600] Loss: 0.0895\n",
      "Epoch [71/600] Loss: 0.0625\n",
      "Epoch [72/600] Loss: 0.1273\n",
      "Epoch [73/600] Loss: 0.8671\n",
      "Epoch [74/600] Loss: 0.0301\n",
      "Epoch [75/600] Loss: 0.4817\n",
      "Epoch [76/600] Loss: 0.2006\n",
      "Epoch [77/600] Loss: 0.0119\n",
      "Epoch [78/600] Loss: 1.1957\n",
      "Epoch [79/600] Loss: 0.0316\n",
      "Epoch [80/600] Loss: 0.0385\n",
      "Epoch [81/600] Loss: 0.4740\n",
      "Epoch [82/600] Loss: 0.3193\n",
      "Epoch [83/600] Loss: 1.1096\n",
      "Epoch [84/600] Loss: 0.0565\n",
      "Epoch [85/600] Loss: 0.0397\n",
      "Epoch [86/600] Loss: 0.0049\n",
      "Epoch [87/600] Loss: 0.0345\n",
      "Epoch [88/600] Loss: 0.0157\n",
      "Epoch [89/600] Loss: 0.0752\n",
      "Epoch [90/600] Loss: 0.0136\n",
      "Epoch [91/600] Loss: 0.0150\n",
      "Epoch [92/600] Loss: 0.0390\n",
      "Epoch [93/600] Loss: 0.0762\n",
      "Epoch [94/600] Loss: 0.0502\n",
      "Epoch [95/600] Loss: 0.0436\n",
      "Epoch [96/600] Loss: 0.1666\n",
      "Epoch [97/600] Loss: 0.0108\n",
      "Epoch [98/600] Loss: 0.0035\n",
      "Epoch [99/600] Loss: 0.0273\n",
      "Epoch [100/600] Loss: 0.0238\n",
      "Epoch [101/600] Loss: 0.1621\n",
      "Epoch [102/600] Loss: 0.0166\n",
      "Epoch [103/600] Loss: 0.0773\n",
      "Epoch [104/600] Loss: 0.2531\n",
      "Epoch [105/600] Loss: 0.0294\n",
      "Epoch [106/600] Loss: 0.0407\n",
      "Epoch [107/600] Loss: 0.0873\n",
      "Epoch [108/600] Loss: 0.0618\n",
      "Epoch [109/600] Loss: 0.0152\n",
      "Epoch [110/600] Loss: 0.0269\n",
      "Epoch [111/600] Loss: 0.0354\n",
      "Epoch [112/600] Loss: 0.0345\n",
      "Epoch [113/600] Loss: 0.0372\n",
      "Epoch [114/600] Loss: 1.4157\n",
      "Epoch [115/600] Loss: 0.0198\n",
      "Epoch [116/600] Loss: 0.0151\n",
      "Epoch [117/600] Loss: 0.0156\n",
      "Epoch [118/600] Loss: 0.0267\n",
      "Epoch [119/600] Loss: 0.0641\n",
      "Epoch [120/600] Loss: 0.4657\n",
      "Epoch [121/600] Loss: 0.0322\n",
      "Epoch [122/600] Loss: 0.0114\n",
      "Epoch [123/600] Loss: 0.0204\n",
      "Epoch [124/600] Loss: 0.0185\n",
      "Epoch [125/600] Loss: 0.0527\n",
      "Epoch [126/600] Loss: 0.0543\n",
      "Epoch [127/600] Loss: 0.0610\n",
      "Epoch [128/600] Loss: 0.0275\n",
      "Epoch [129/600] Loss: 0.0086\n",
      "Epoch [130/600] Loss: 0.0069\n",
      "Epoch [131/600] Loss: 0.2324\n",
      "Epoch [132/600] Loss: 0.0843\n",
      "Epoch [133/600] Loss: 0.0335\n",
      "Epoch [134/600] Loss: 0.0434\n",
      "Epoch [135/600] Loss: 0.0190\n",
      "Epoch [136/600] Loss: 0.0442\n",
      "Epoch [137/600] Loss: 0.1541\n",
      "Epoch [138/600] Loss: 0.0067\n",
      "Epoch [139/600] Loss: 0.0402\n",
      "Epoch [140/600] Loss: 0.0067\n",
      "Epoch [141/600] Loss: 0.0133\n",
      "Epoch [142/600] Loss: 0.0271\n",
      "Epoch [143/600] Loss: 0.3506\n",
      "Epoch [144/600] Loss: 0.0236\n",
      "Epoch [145/600] Loss: 0.1943\n",
      "Epoch [146/600] Loss: 0.0271\n",
      "Epoch [147/600] Loss: 0.0474\n",
      "Epoch [148/600] Loss: 0.0128\n",
      "Epoch [149/600] Loss: 0.1161\n",
      "Epoch [150/600] Loss: 0.4626\n",
      "Epoch [151/600] Loss: 0.0046\n",
      "Epoch [152/600] Loss: 0.0381\n",
      "Epoch [153/600] Loss: 0.0140\n",
      "Epoch [154/600] Loss: 0.5241\n",
      "Epoch [155/600] Loss: 0.1152\n",
      "Epoch [156/600] Loss: 0.0096\n",
      "Epoch [157/600] Loss: 0.1431\n",
      "Epoch [158/600] Loss: 0.1231\n",
      "Epoch [159/600] Loss: 0.0062\n",
      "Epoch [160/600] Loss: 0.0139\n",
      "Epoch [161/600] Loss: 0.0711\n",
      "Epoch [162/600] Loss: 0.0246\n",
      "Epoch [163/600] Loss: 0.0742\n",
      "Epoch [164/600] Loss: 0.1281\n",
      "Epoch [165/600] Loss: 0.0511\n",
      "Epoch [166/600] Loss: 0.0335\n",
      "Epoch [167/600] Loss: 0.0035\n",
      "Epoch [168/600] Loss: 0.1147\n",
      "Epoch [169/600] Loss: 0.0101\n",
      "Epoch [170/600] Loss: 0.0432\n",
      "Epoch [171/600] Loss: 0.0225\n",
      "Epoch [172/600] Loss: 0.0279\n",
      "Epoch [173/600] Loss: 0.0506\n",
      "Epoch [174/600] Loss: 0.0281\n",
      "Epoch [175/600] Loss: 0.0801\n",
      "Epoch [176/600] Loss: 0.0975\n",
      "Epoch [177/600] Loss: 0.0812\n",
      "Epoch [178/600] Loss: 0.0770\n",
      "Epoch [179/600] Loss: 0.3775\n",
      "Epoch [180/600] Loss: 0.1618\n",
      "Epoch [181/600] Loss: 0.0387\n",
      "Epoch [182/600] Loss: 0.0033\n",
      "Epoch [183/600] Loss: 0.0055\n",
      "Epoch [184/600] Loss: 0.0147\n",
      "Epoch [185/600] Loss: 0.0829\n",
      "Epoch [186/600] Loss: 0.0202\n",
      "Epoch [187/600] Loss: 0.2088\n",
      "Epoch [188/600] Loss: 0.4681\n",
      "Epoch [189/600] Loss: 0.0163\n",
      "Epoch [190/600] Loss: 0.3791\n",
      "Epoch [191/600] Loss: 0.0331\n",
      "Epoch [192/600] Loss: 0.0022\n",
      "Epoch [193/600] Loss: 0.0120\n",
      "Epoch [194/600] Loss: 0.0296\n",
      "Epoch [195/600] Loss: 0.0070\n",
      "Epoch [196/600] Loss: 0.0255\n",
      "Epoch [197/600] Loss: 0.0956\n",
      "Epoch [198/600] Loss: 0.3079\n",
      "Epoch [199/600] Loss: 0.0286\n",
      "Epoch [200/600] Loss: 0.1029\n",
      "Epoch [201/600] Loss: 0.0250\n",
      "Epoch [202/600] Loss: 0.0183\n",
      "Epoch [203/600] Loss: 0.2386\n",
      "Epoch [204/600] Loss: 0.0175\n",
      "Epoch [205/600] Loss: 0.0323\n",
      "Epoch [206/600] Loss: 0.0288\n",
      "Epoch [207/600] Loss: 0.0348\n",
      "Epoch [208/600] Loss: 0.0247\n",
      "Epoch [209/600] Loss: 0.0328\n",
      "Epoch [210/600] Loss: 0.0584\n",
      "Epoch [211/600] Loss: 0.0063\n",
      "Epoch [212/600] Loss: 0.0206\n",
      "Epoch [213/600] Loss: 0.0221\n",
      "Epoch [214/600] Loss: 0.0315\n",
      "Epoch [215/600] Loss: 0.2817\n",
      "Epoch [216/600] Loss: 0.0667\n",
      "Epoch [217/600] Loss: 0.0148\n",
      "Epoch [218/600] Loss: 0.0133\n",
      "Epoch [219/600] Loss: 0.0703\n",
      "Epoch [220/600] Loss: 0.0407\n",
      "Epoch [221/600] Loss: 0.0349\n",
      "Epoch [222/600] Loss: 0.1619\n",
      "Epoch [223/600] Loss: 0.0600\n",
      "Epoch [224/600] Loss: 0.1743\n",
      "Epoch [225/600] Loss: 0.1370\n",
      "Epoch [226/600] Loss: 0.0375\n",
      "Epoch [227/600] Loss: 0.7232\n",
      "Epoch [228/600] Loss: 0.0898\n",
      "Epoch [229/600] Loss: 0.4383\n",
      "Epoch [230/600] Loss: 0.0258\n",
      "Epoch [231/600] Loss: 0.0118\n",
      "Epoch [232/600] Loss: 0.0111\n",
      "Epoch [233/600] Loss: 0.8048\n",
      "Epoch [234/600] Loss: 0.0918\n",
      "Epoch [235/600] Loss: 0.0466\n",
      "Epoch [236/600] Loss: 0.0137\n",
      "Epoch [237/600] Loss: 0.1606\n",
      "Epoch [238/600] Loss: 0.0681\n",
      "Epoch [239/600] Loss: 0.8752\n",
      "Epoch [240/600] Loss: 0.3028\n",
      "Epoch [241/600] Loss: 0.0305\n",
      "Epoch [242/600] Loss: 0.1937\n",
      "Epoch [243/600] Loss: 0.0090\n",
      "Epoch [244/600] Loss: 0.0236\n",
      "Epoch [245/600] Loss: 0.4663\n",
      "Epoch [246/600] Loss: 0.0558\n",
      "Epoch [247/600] Loss: 0.1589\n",
      "Epoch [248/600] Loss: 0.2826\n",
      "Epoch [249/600] Loss: 0.0236\n",
      "Epoch [250/600] Loss: 0.0078\n",
      "Epoch [251/600] Loss: 0.0034\n",
      "Epoch [252/600] Loss: 0.0318\n",
      "Epoch [253/600] Loss: 0.0231\n",
      "Epoch [254/600] Loss: 0.0032\n",
      "Epoch [255/600] Loss: 0.0815\n",
      "Epoch [256/600] Loss: 0.0070\n",
      "Epoch [257/600] Loss: 0.0176\n",
      "Epoch [258/600] Loss: 0.0156\n",
      "Epoch [259/600] Loss: 0.0022\n",
      "Epoch [260/600] Loss: 0.0983\n",
      "Epoch [261/600] Loss: 0.3770\n",
      "Epoch [262/600] Loss: 0.0745\n",
      "Epoch [263/600] Loss: 0.4599\n",
      "Epoch [264/600] Loss: 0.0114\n",
      "Epoch [265/600] Loss: 0.2677\n",
      "Epoch [266/600] Loss: 0.0680\n",
      "Epoch [267/600] Loss: 0.0024\n",
      "Epoch [268/600] Loss: 0.0101\n",
      "Epoch [269/600] Loss: 0.0096\n",
      "Epoch [270/600] Loss: 0.2313\n",
      "Epoch [271/600] Loss: 0.0384\n",
      "Epoch [272/600] Loss: 0.0596\n",
      "Epoch [273/600] Loss: 0.0899\n",
      "Epoch [274/600] Loss: 0.0433\n",
      "Epoch [275/600] Loss: 0.0514\n",
      "Epoch [276/600] Loss: 0.0246\n",
      "Epoch [277/600] Loss: 0.5342\n",
      "Epoch [278/600] Loss: 0.2184\n",
      "Epoch [279/600] Loss: 0.7131\n",
      "Epoch [280/600] Loss: 0.0029\n",
      "Epoch [281/600] Loss: 0.0118\n",
      "Epoch [282/600] Loss: 0.8828\n",
      "Epoch [283/600] Loss: 0.0711\n",
      "Epoch [284/600] Loss: 0.0136\n",
      "Epoch [285/600] Loss: 0.3853\n",
      "Epoch [286/600] Loss: 0.0050\n",
      "Epoch [287/600] Loss: 0.0723\n",
      "Epoch [288/600] Loss: 0.0198\n",
      "Epoch [289/600] Loss: 0.1972\n",
      "Epoch [290/600] Loss: 0.0198\n",
      "Epoch [291/600] Loss: 0.3520\n",
      "Epoch [292/600] Loss: 0.0088\n",
      "Epoch [293/600] Loss: 0.8227\n",
      "Epoch [294/600] Loss: 0.0232\n",
      "Epoch [295/600] Loss: 1.0498\n",
      "Epoch [296/600] Loss: 0.0356\n",
      "Epoch [297/600] Loss: 0.0049\n",
      "Epoch [298/600] Loss: 0.0064\n",
      "Epoch [299/600] Loss: 1.2014\n",
      "Epoch [300/600] Loss: 0.0038\n",
      "Epoch [301/600] Loss: 0.0670\n",
      "Epoch [302/600] Loss: 0.0031\n",
      "Epoch [303/600] Loss: 0.0028\n",
      "Epoch [304/600] Loss: 0.4532\n",
      "Epoch [305/600] Loss: 0.0260\n",
      "Epoch [306/600] Loss: 0.0379\n",
      "Epoch [307/600] Loss: 0.0544\n",
      "Epoch [308/600] Loss: 0.0226\n",
      "Epoch [309/600] Loss: 0.0522\n",
      "Epoch [310/600] Loss: 0.0954\n",
      "Epoch [311/600] Loss: 0.0427\n",
      "Epoch [312/600] Loss: 0.0236\n",
      "Epoch [313/600] Loss: 0.6233\n",
      "Epoch [314/600] Loss: 0.1291\n",
      "Epoch [315/600] Loss: 0.0865\n",
      "Epoch [316/600] Loss: 0.0494\n",
      "Epoch [317/600] Loss: 0.6121\n",
      "Epoch [318/600] Loss: 0.0406\n",
      "Epoch [319/600] Loss: 0.0112\n",
      "Epoch [320/600] Loss: 0.0223\n",
      "Epoch [321/600] Loss: 0.0067\n",
      "Epoch [322/600] Loss: 0.0036\n",
      "Epoch [323/600] Loss: 0.0271\n",
      "Epoch [324/600] Loss: 0.0301\n",
      "Epoch [325/600] Loss: 0.3410\n",
      "Epoch [326/600] Loss: 0.0576\n",
      "Epoch [327/600] Loss: 1.1635\n",
      "Epoch [328/600] Loss: 0.0612\n",
      "Epoch [329/600] Loss: 0.0168\n",
      "Epoch [330/600] Loss: 0.0587\n",
      "Epoch [331/600] Loss: 0.0995\n",
      "Epoch [332/600] Loss: 0.0180\n",
      "Epoch [333/600] Loss: 0.0834\n",
      "Epoch [334/600] Loss: 0.0201\n",
      "Epoch [335/600] Loss: 0.0573\n",
      "Epoch [336/600] Loss: 0.0280\n",
      "Epoch [337/600] Loss: 0.1206\n",
      "Epoch [338/600] Loss: 0.1555\n",
      "Epoch [339/600] Loss: 0.0917\n",
      "Epoch [340/600] Loss: 0.0474\n",
      "Epoch [341/600] Loss: 0.8310\n",
      "Epoch [342/600] Loss: 0.0491\n",
      "Epoch [343/600] Loss: 0.0280\n",
      "Epoch [344/600] Loss: 0.2567\n",
      "Epoch [345/600] Loss: 0.0447\n",
      "Epoch [346/600] Loss: 0.0228\n",
      "Epoch [347/600] Loss: 0.5847\n",
      "Epoch [348/600] Loss: 0.0422\n",
      "Epoch [349/600] Loss: 0.0157\n",
      "Epoch [350/600] Loss: 0.0390\n",
      "Epoch [351/600] Loss: 0.0282\n",
      "Epoch [352/600] Loss: 0.0094\n",
      "Epoch [353/600] Loss: 1.7562\n",
      "Epoch [354/600] Loss: 1.2659\n",
      "Epoch [355/600] Loss: 0.0651\n",
      "Epoch [356/600] Loss: 0.0151\n",
      "Epoch [357/600] Loss: 0.0385\n",
      "Epoch [358/600] Loss: 0.0613\n",
      "Epoch [359/600] Loss: 0.0937\n",
      "Epoch [360/600] Loss: 0.0624\n",
      "Epoch [361/600] Loss: 0.0167\n",
      "Epoch [362/600] Loss: 0.0401\n",
      "Epoch [363/600] Loss: 0.0794\n",
      "Epoch [364/600] Loss: 0.2916\n",
      "Epoch [365/600] Loss: 0.0478\n",
      "Epoch [366/600] Loss: 0.2086\n",
      "Epoch [367/600] Loss: 0.0133\n",
      "Epoch [368/600] Loss: 0.1388\n",
      "Epoch [369/600] Loss: 0.1559\n",
      "Epoch [370/600] Loss: 0.0527\n",
      "Epoch [371/600] Loss: 0.1049\n",
      "Epoch [372/600] Loss: 0.0968\n",
      "Epoch [373/600] Loss: 0.8743\n",
      "Epoch [374/600] Loss: 0.0205\n",
      "Epoch [375/600] Loss: 0.0770\n",
      "Epoch [376/600] Loss: 0.0270\n",
      "Epoch [377/600] Loss: 0.0379\n",
      "Epoch [378/600] Loss: 0.0722\n",
      "Epoch [379/600] Loss: 0.0178\n",
      "Epoch [380/600] Loss: 0.0050\n",
      "Epoch [381/600] Loss: 0.0890\n",
      "Epoch [382/600] Loss: 0.0632\n",
      "Epoch [383/600] Loss: 0.1743\n",
      "Epoch [384/600] Loss: 0.0380\n",
      "Epoch [385/600] Loss: 0.0282\n",
      "Epoch [386/600] Loss: 0.0458\n",
      "Epoch [387/600] Loss: 0.0195\n",
      "Epoch [388/600] Loss: 0.0619\n",
      "Epoch [389/600] Loss: 0.1222\n",
      "Epoch [390/600] Loss: 0.1260\n",
      "Epoch [391/600] Loss: 0.7378\n",
      "Epoch [392/600] Loss: 0.1014\n",
      "Epoch [393/600] Loss: 0.0001\n",
      "Epoch [394/600] Loss: 0.0335\n",
      "Epoch [395/600] Loss: 0.0742\n",
      "Epoch [396/600] Loss: 0.0261\n",
      "Epoch [397/600] Loss: 0.0021\n",
      "Epoch [398/600] Loss: 0.0387\n",
      "Epoch [399/600] Loss: 0.1436\n",
      "Epoch [400/600] Loss: 0.0148\n",
      "Epoch [401/600] Loss: 0.4658\n",
      "Epoch [402/600] Loss: 0.0226\n",
      "Epoch [403/600] Loss: 0.0297\n",
      "Epoch [404/600] Loss: 0.0260\n",
      "Epoch [405/600] Loss: 0.1429\n",
      "Epoch [406/600] Loss: 0.0854\n",
      "Epoch [407/600] Loss: 0.4434\n",
      "Epoch [408/600] Loss: 0.0119\n",
      "Epoch [409/600] Loss: 0.0023\n",
      "Epoch [410/600] Loss: 0.0235\n",
      "Epoch [411/600] Loss: 0.0151\n",
      "Epoch [412/600] Loss: 0.1116\n",
      "Epoch [413/600] Loss: 0.0344\n",
      "Epoch [414/600] Loss: 0.0309\n",
      "Epoch [415/600] Loss: 0.0416\n",
      "Epoch [416/600] Loss: 0.0633\n",
      "Epoch [417/600] Loss: 0.0560\n",
      "Epoch [418/600] Loss: 0.4019\n",
      "Epoch [419/600] Loss: 0.6431\n",
      "Epoch [420/600] Loss: 0.0117\n",
      "Epoch [421/600] Loss: 0.6122\n",
      "Epoch [422/600] Loss: 0.9129\n",
      "Epoch [423/600] Loss: 0.0397\n",
      "Epoch [424/600] Loss: 0.0084\n",
      "Epoch [425/600] Loss: 0.0015\n",
      "Epoch [426/600] Loss: 0.0376\n",
      "Epoch [427/600] Loss: 0.2871\n",
      "Epoch [428/600] Loss: 0.6836\n",
      "Epoch [429/600] Loss: 0.0152\n",
      "Epoch [430/600] Loss: 0.1469\n",
      "Epoch [431/600] Loss: 0.0620\n",
      "Epoch [432/600] Loss: 0.6704\n",
      "Epoch [433/600] Loss: 0.0192\n",
      "Epoch [434/600] Loss: 0.0105\n",
      "Epoch [435/600] Loss: 0.0375\n",
      "Epoch [436/600] Loss: 0.0926\n",
      "Epoch [437/600] Loss: 0.5736\n",
      "Epoch [438/600] Loss: 0.0363\n",
      "Epoch [439/600] Loss: 0.0218\n",
      "Epoch [440/600] Loss: 0.0202\n",
      "Epoch [441/600] Loss: 0.1172\n",
      "Epoch [442/600] Loss: 0.0977\n",
      "Epoch [443/600] Loss: 0.0178\n",
      "Epoch [444/600] Loss: 0.0105\n",
      "Epoch [445/600] Loss: 0.0042\n",
      "Epoch [446/600] Loss: 1.0826\n",
      "Epoch [447/600] Loss: 0.0504\n",
      "Epoch [448/600] Loss: 0.1888\n",
      "Epoch [449/600] Loss: 0.0367\n",
      "Epoch [450/600] Loss: 0.4906\n",
      "Epoch [451/600] Loss: 0.0690\n",
      "Epoch [452/600] Loss: 0.0091\n",
      "Epoch [453/600] Loss: 0.0955\n",
      "Epoch [454/600] Loss: 0.0534\n",
      "Epoch [455/600] Loss: 0.0331\n",
      "Epoch [456/600] Loss: 0.0381\n",
      "Epoch [457/600] Loss: 0.0287\n",
      "Epoch [458/600] Loss: 0.0572\n",
      "Epoch [459/600] Loss: 0.0230\n",
      "Epoch [460/600] Loss: 0.5334\n",
      "Epoch [461/600] Loss: 0.0046\n",
      "Epoch [462/600] Loss: 0.1078\n",
      "Epoch [463/600] Loss: 0.0232\n",
      "Epoch [464/600] Loss: 0.0205\n",
      "Epoch [465/600] Loss: 0.1255\n",
      "Epoch [466/600] Loss: 0.0158\n",
      "Epoch [467/600] Loss: 0.0441\n",
      "Epoch [468/600] Loss: 0.0784\n",
      "Epoch [469/600] Loss: 0.0057\n",
      "Epoch [470/600] Loss: 0.2263\n",
      "Epoch [471/600] Loss: 0.0147\n",
      "Epoch [472/600] Loss: 0.0090\n",
      "Epoch [473/600] Loss: 1.3085\n",
      "Epoch [474/600] Loss: 0.3942\n",
      "Epoch [475/600] Loss: 0.0139\n",
      "Epoch [476/600] Loss: 0.0455\n",
      "Epoch [477/600] Loss: 0.0083\n",
      "Epoch [478/600] Loss: 0.4416\n",
      "Epoch [479/600] Loss: 0.1995\n",
      "Epoch [480/600] Loss: 0.0085\n",
      "Epoch [481/600] Loss: 0.0146\n",
      "Epoch [482/600] Loss: 0.0945\n",
      "Epoch [483/600] Loss: 0.1678\n",
      "Epoch [484/600] Loss: 0.0425\n",
      "Epoch [485/600] Loss: 0.0062\n",
      "Epoch [486/600] Loss: 0.2379\n",
      "Epoch [487/600] Loss: 0.0389\n",
      "Epoch [488/600] Loss: 0.6147\n",
      "Epoch [489/600] Loss: 0.0290\n",
      "Epoch [490/600] Loss: 0.0159\n",
      "Epoch [491/600] Loss: 1.6809\n",
      "Epoch [492/600] Loss: 0.0646\n",
      "Epoch [493/600] Loss: 0.7227\n",
      "Epoch [494/600] Loss: 0.0065\n",
      "Epoch [495/600] Loss: 0.0221\n",
      "Epoch [496/600] Loss: 0.0713\n",
      "Epoch [497/600] Loss: 0.0020\n",
      "Epoch [498/600] Loss: 0.0381\n",
      "Epoch [499/600] Loss: 0.0061\n",
      "Epoch [500/600] Loss: 0.0182\n",
      "Epoch [501/600] Loss: 0.0387\n",
      "Epoch [502/600] Loss: 0.1405\n",
      "Epoch [503/600] Loss: 0.0518\n",
      "Epoch [504/600] Loss: 0.2968\n",
      "Epoch [505/600] Loss: 0.0050\n",
      "Epoch [506/600] Loss: 0.0246\n",
      "Epoch [507/600] Loss: 0.5066\n",
      "Epoch [508/600] Loss: 0.4224\n",
      "Epoch [509/600] Loss: 0.0355\n",
      "Epoch [510/600] Loss: 0.0177\n",
      "Epoch [511/600] Loss: 0.0338\n",
      "Epoch [512/600] Loss: 0.0049\n",
      "Epoch [513/600] Loss: 0.8102\n",
      "Epoch [514/600] Loss: 0.0114\n",
      "Epoch [515/600] Loss: 0.0204\n",
      "Epoch [516/600] Loss: 0.0202\n",
      "Epoch [517/600] Loss: 0.0680\n",
      "Epoch [518/600] Loss: 0.0097\n",
      "Epoch [519/600] Loss: 0.0075\n",
      "Epoch [520/600] Loss: 0.0253\n",
      "Epoch [521/600] Loss: 0.0641\n",
      "Epoch [522/600] Loss: 0.3442\n",
      "Epoch [523/600] Loss: 0.0101\n",
      "Epoch [524/600] Loss: 0.0368\n",
      "Epoch [525/600] Loss: 0.0217\n",
      "Epoch [526/600] Loss: 0.0210\n",
      "Epoch [527/600] Loss: 0.0015\n",
      "Epoch [528/600] Loss: 0.1199\n",
      "Epoch [529/600] Loss: 0.0103\n",
      "Epoch [530/600] Loss: 0.0377\n",
      "Epoch [531/600] Loss: 0.0019\n",
      "Epoch [532/600] Loss: 0.8221\n",
      "Epoch [533/600] Loss: 0.0435\n",
      "Epoch [534/600] Loss: 0.0179\n",
      "Epoch [535/600] Loss: 0.3139\n",
      "Epoch [536/600] Loss: 0.1197\n",
      "Epoch [537/600] Loss: 0.1976\n",
      "Epoch [538/600] Loss: 0.0022\n",
      "Epoch [539/600] Loss: 0.0031\n",
      "Epoch [540/600] Loss: 0.0040\n",
      "Epoch [541/600] Loss: 0.0121\n",
      "Epoch [542/600] Loss: 0.0229\n",
      "Epoch [543/600] Loss: 0.0107\n",
      "Epoch [544/600] Loss: 0.0092\n",
      "Epoch [545/600] Loss: 0.0825\n",
      "Epoch [546/600] Loss: 0.0330\n",
      "Epoch [547/600] Loss: 0.0637\n",
      "Epoch [548/600] Loss: 0.0035\n",
      "Epoch [549/600] Loss: 0.0158\n",
      "Epoch [550/600] Loss: 0.0131\n",
      "Epoch [551/600] Loss: 0.0117\n",
      "Epoch [552/600] Loss: 0.0652\n",
      "Epoch [553/600] Loss: 0.0140\n",
      "Epoch [554/600] Loss: 0.2587\n",
      "Epoch [555/600] Loss: 0.0428\n",
      "Epoch [556/600] Loss: 0.0247\n",
      "Epoch [557/600] Loss: 0.0372\n",
      "Epoch [558/600] Loss: 0.0625\n",
      "Epoch [559/600] Loss: 0.0665\n",
      "Epoch [560/600] Loss: 0.1435\n",
      "Epoch [561/600] Loss: 0.0346\n",
      "Epoch [562/600] Loss: 0.1016\n",
      "Epoch [563/600] Loss: 0.0234\n",
      "Epoch [564/600] Loss: 0.0388\n",
      "Epoch [565/600] Loss: 0.5746\n",
      "Epoch [566/600] Loss: 0.0361\n",
      "Epoch [567/600] Loss: 0.0192\n",
      "Epoch [568/600] Loss: 0.0352\n",
      "Epoch [569/600] Loss: 0.1180\n",
      "Epoch [570/600] Loss: 0.0585\n",
      "Epoch [571/600] Loss: 0.0309\n",
      "Epoch [572/600] Loss: 0.0088\n",
      "Epoch [573/600] Loss: 0.0367\n",
      "Epoch [574/600] Loss: 0.0215\n",
      "Epoch [575/600] Loss: 0.1304\n",
      "Epoch [576/600] Loss: 0.0492\n",
      "Epoch [577/600] Loss: 0.3316\n",
      "Epoch [578/600] Loss: 0.2504\n",
      "Epoch [579/600] Loss: 0.0884\n",
      "Epoch [580/600] Loss: 0.2194\n",
      "Epoch [581/600] Loss: 0.0220\n",
      "Epoch [582/600] Loss: 0.1426\n",
      "Epoch [583/600] Loss: 0.8556\n",
      "Epoch [584/600] Loss: 0.7166\n",
      "Epoch [585/600] Loss: 0.5298\n",
      "Epoch [586/600] Loss: 0.1133\n",
      "Epoch [587/600] Loss: 0.0039\n",
      "Epoch [588/600] Loss: 0.0093\n",
      "Epoch [589/600] Loss: 0.0080\n",
      "Epoch [590/600] Loss: 0.0257\n",
      "Epoch [591/600] Loss: 0.0334\n",
      "Epoch [592/600] Loss: 0.0374\n",
      "Epoch [593/600] Loss: 0.0762\n",
      "Epoch [594/600] Loss: 0.2311\n",
      "Epoch [595/600] Loss: 0.0399\n",
      "Epoch [596/600] Loss: 0.0100\n",
      "Epoch [597/600] Loss: 0.0441\n",
      "Epoch [598/600] Loss: 0.0662\n",
      "Epoch [599/600] Loss: 0.0019\n",
      "Epoch [600/600] Loss: 0.0547\n",
      "\n",
      "Validation Metrics:\n",
      "Precision: 0.4312617702448211\n",
      "Recall: 0.10904761904761905\n",
      "F1 Score: 0.1740782972253896\n",
      "ROC AUC Score: 0.8385175253121062\n",
      "Log Loss: 0.16960743434270065\n",
      "Zero-One Loss: 0.05432092593055521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryClassificationNN(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=49, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_validate(x_train_data,y_train_data,x_valid_data,y_valid_data,epochs=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4294efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
